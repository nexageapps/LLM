{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nexageapps/LLM/blob/main/LLM_%7C_Basic_%7C_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9T5bk-_wIkWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5d990a-d5db-4cc3-b029-6827fb322efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Your max_length is set to 142, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot results: {'sequence': 'I am so happy with this new feature at nexageapps.com', 'labels': ['feature request', 'user feedback', 'bug report'], 'scores': [0.6356408596038818, 0.2891876995563507, 0.07517150044441223]}\n",
            "Sentiment results: [{'label': 'POSITIVE', 'score': 0.9998321533203125}]\n",
            "Generate results: [{'generated_text': 'In nexageapps.com, we will teach you how to use nexage apps in the future.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]\n",
            "NER results: [{'entity_group': 'PER', 'score': np.float32(0.98900837), 'word': 'Karthik Arjun', 'start': 11, 'end': 24}, {'entity_group': 'MISC', 'score': np.float32(0.4461315), 'word': 'AI', 'start': 45, 'end': 47}, {'entity_group': 'ORG', 'score': np.float32(0.9948821), 'word': 'University of Auckland', 'start': 59, 'end': 81}, {'entity_group': 'ORG', 'score': np.float32(0.9936502), 'word': 'Deloitte', 'start': 93, 'end': 101}, {'entity_group': 'LOC', 'score': np.float32(0.9986615), 'word': 'NZ', 'start': 103, 'end': 105}]\n",
            "Q&A results: {'score': 0.41897669434547424, 'start': 93, 'end': 105, 'answer': 'Deloitte, NZ'}\n",
            "Summarize results: [{'summary_text': \" Karthik Arjun is a Master's in AI student at University of Auckland . He has 12.5 years of SAP experience . He is a certified SAP BTP & Integration Architect! He is also an SAP expert and a certified BTP and Integration Architect . He works at Deloitte, NZ!\"}]\n",
            "Translator results: [{'translation_text': 'This course is produced by Hugging Face.'}]\n"
          ]
        }
      ],
      "source": [
        "# Author: Karthik Arjun\n",
        "# LinkedIn: https://www.linkedin.com/in/karthik-arjun-a5b4a258/\n",
        "# Comment: Beginner level understanding about LLM\n",
        "# Created at: 19/11/2025\n",
        "# Updated at: 12/01/2026\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "#Transformers: Neural networks architecture to understand the relationship in data (e.g., Natural language).\n",
        " # (Analogy: Like Lord Brahma having a broad vision to understand everything happening in creation.)\n",
        "\n",
        "#Classifiers: Algorithms that assign data points to specific categories (e.g., spam vs. not spam).\n",
        " # (Analogy: Like Brahma deciding which category each being belongs to — humans, animals, etc.)\n",
        "\n",
        "#Pipelines: Building process of data flow. (e.g., cleaning -> transform -> predict)\n",
        " # (Analogy: Brahma shapes creation through sequential steps — form → features → life.)\n",
        "\n",
        "# 1. Define the pipelines\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "sentiment_classifier = pipeline(\"sentiment-analysis\")\n",
        "generator_classifier = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "ner_classifier = pipeline(\"ner\", grouped_entities=True) #Named Entity Recognition\n",
        "qa_classifier = pipeline(\"question-answering\") #Question and Answer\n",
        "sumarize_classifier = pipeline(\"summarization\")\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "\n",
        "# 2. Get user input\n",
        "text = \"I am so happy with this new feature at nexageapps.com\"\n",
        "labels = [\"feature request\", \"user feedback\", \"bug report\"]\n",
        "generate = \"In nexageapps.com, we will teach you how to\"\n",
        "name = \"My name is Karthik Arjun. I am a Master's in AI student at University of Auckland. I work at Deloitte, NZ!\"\n",
        "question = \"Where do I work?\"\n",
        "sumarize = name + \" He has 12.5 years of SAP experience. He is a certified SAP BTP & Integration Architect!\"\n",
        "translator_text = \"Ce cours est produit par Karthik. Refer from HF portal\"\n",
        "\n",
        "# 3. Get results from pipelines\n",
        "zero_shot_results = zero_shot_classifier(text, labels)\n",
        "sentiment_results = sentiment_classifier(text)\n",
        "generate_results = generator_classifier(generate, max_length=30)\n",
        "ner_results = ner_classifier(name)\n",
        "qa_results = qa_classifier(question=question, context=name)\n",
        "sumarize_results = sumarize_classifier(sumarize)\n",
        "translator_results = translator(translator_text)\n",
        "\n",
        "# 4. Process the results as needed in your program\n",
        "print(\"Zero-shot results:\", zero_shot_results)\n",
        "print(\"Sentiment results:\", sentiment_results)\n",
        "print(\"Generate results:\", generate_results)\n",
        "print(\"NER results:\", ner_results)\n",
        "print(\"Q&A results:\", qa_results)\n",
        "print(\"Summarize results:\", sumarize_results)\n",
        "print(\"Translator results:\", translator_results)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}