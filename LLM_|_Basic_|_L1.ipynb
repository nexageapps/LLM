{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7G2nmpmuMZ+Qb/7XCyy1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nexageapps/LLM/blob/main/LLM_%7C_Basic_%7C_L1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EkgCnD0oNVjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d0152f-c28a-454c-c729-e9801c320b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
            "4690\n",
            "1130\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n",
            "Input Tokens: ['I', 'had', 'come']\n",
            "Encoded IDs: [53, 514, 277]\n",
            "Decoded Text: I had come\n"
          ]
        }
      ],
      "source": [
        "# Author: Karthik Arjun\n",
        "# LinkedIn: https://www.linkedin.com/in/karthik-arjun-a5b4a258/\n",
        "# Comment: Beginner level understanding about LLM\n",
        "# Book Reference: Build a LLM - By Sebastian\n",
        "# Chapter - #2: Working with text data\n",
        "\n",
        "#Step-1: Import the library functions\n",
        "import os #Helps interact with the operating system — like checking if a file exists.\n",
        "import urllib.request #Allows you to download files and interact with URLs.\n",
        "import re #Python’s regular expressions module.\n",
        "\n",
        "#Step-2: Reference\n",
        "if not os.path.exists(\"the-verdict.txt\"): #File exist??\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path) #This downloads the file from the URL and saves it as the-verdict.txt in your local folder\n",
        "\n",
        "#Step-3: Action : Create Tokens\n",
        "    with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "      raw_text = f.read()\n",
        "\n",
        "    #open(...) opens the file in read mode (\"r\").\n",
        "    #The with statement makes sure the file is automatically closed after you're done using it\n",
        "    #f is the file object.\n",
        "    #.read() reads the entire contents of the file into a variable named raw_text.\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    #split by using the regular expression include all punchuations\n",
        "    #.strip() removes leading and trailing whitespace.\n",
        "    #if item.strip() removes empty strings or strings with just spaces.\n",
        "\n",
        "\n",
        "#Step-4: Execution #1\n",
        "print(\"Total number of character:\", len(raw_text)) #Print the length of the total content\n",
        "print(raw_text[:99]) #Print the first 99 Char\n",
        "print(preprocessed[:30]) #Print the first 30 tokens from the processed text.\n",
        "print(len(preprocessed))\n",
        "\n",
        "#Step-6: Action: Create Token ID - Encoder\n",
        "all_words =  sorted(set(preprocessed)) #set comman removes duplicate values\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)\n",
        "\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)} #create dictionaries\n",
        "#create a dictionary that maps each unique token (word or punctuation) to a unique integer ID\n",
        "#enumerate:adds index to the value\n",
        "for i , item in enumerate(vocab.items()):\n",
        "  print (item)\n",
        "  if i >= 50:\n",
        "    break\n",
        "\n",
        "#Step-7: Decode the messages using the tokens with other sentenses!\n",
        "\n",
        "# Sample input sentence\n",
        "sample_text = \"I had come\"\n",
        "\n",
        "# Tokenize the input in the same way\n",
        "input_tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', sample_text)\n",
        "input_tokens = [item.strip() for item in input_tokens if item.strip()]\n",
        "print(\"Input Tokens:\", input_tokens)\n",
        "\n",
        "# Encode using vocab\n",
        "encoded = [vocab[token] for token in input_tokens if token in vocab]\n",
        "print(\"Encoded IDs:\", encoded)\n",
        "\n",
        "# Create reverse mapping\n",
        "id2token = {idx: tok for tok, idx in vocab.items()}\n",
        "\n",
        "# Decode\n",
        "decoded = [id2token[idx] for idx in encoded]\n",
        "print(\"Decoded Text:\", ' '.join(decoded))\n",
        "\n",
        "#Step-Last: Questions\n",
        "\n",
        "#Q1: Total number of character is 20479, then why we have 4690 tokens?\n",
        "#A1: Characters ≠ Tokens\n",
        "    #A character is a single symbol: a letter, number, space, punctuation, etc.\n",
        "    #A token is usually a meaningful chunk, like a word or punctuation mark.\n",
        "      #Splits on whitespace and punctuation, but keeps those as tokens.\n",
        "      #Then removes empty tokens and trims spaces.\n",
        "#----------------------------------------------------------------------------#\n"
      ]
    }
  ]
}